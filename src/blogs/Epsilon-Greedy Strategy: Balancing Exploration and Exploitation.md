# Epsilon-Greedy Strategy: Balancing Exploration and Exploitation

---

date: 08/01/2025
topics: python algorithm machine_learning

---

## Background

When I was doing research on RAG (Retrieval-Augmented Generation), I noticed that some of the retrieval strategies are determined by reinforcement learning. A fundamental challenge in such scenarios is the **exploration-exploitation dilemma**, where you must decide whether to explore new possibilities or exploit the best-known option. To prevent getting stuck in a local optimum, the **Epsilon-Greedy Strategy** balances exploration and exploitation.

## Epsilon-Greedy Strategy

To balance exploration and exploitation, the probability of exploration is set to \(\epsilon\), while the probability of exploitation is \(1-\epsilon\). This is similar to mutation in genetic algorithms, which provides the possibility of finding a global solution. The Epsilon-Greedy Strategy can be used in multi-armed bandit problems, recommendation systems, and more.

## Example

Consider a scenario with several slot machines (bandits), each with an unknown probability of paying out. The goal is to maximize rewards by figuring out which machine is the most profitable.

We simulate this scenario with 5 slot machines, each having a different probability of payout, and observe how the Epsilon-Greedy Strategy performs over 1000 pulls.

### Example Code

```python
import numpy as np

true_probs = [0.1, 0.4, 0.02, 0.18, 0.7]

num_bandits = len(true_probs)
epsilon = 0.1   # 10% of the time we will explore
num_pulls = 1000   # number of times we will pull the arm
total_reward = 0
num_times_pulled = np.zeros(
    len(true_probs)
)   # number of times each arm is pulled
estimated_probs = np.zeros(
    len(true_probs)
)   # estimated probability of each arm

for i in range(num_pulls):
    if np.random.rand() < epsilon:
        chosen_bandit = np.random.choice(num_bandits)
    else:
        chosen_bandit = np.argmax(estimated_probs)

    # simulate pulling
    reward = np.random.rand() < true_probs[chosen_bandit]
    total_reward += reward

    # record
    num_times_pulled[chosen_bandit] += 1
    estimated_probs[chosen_bandit] += (
        reward - estimated_probs[chosen_bandit]
    ) / num_times_pulled[chosen_bandit]

print(f'Estimated probs: {estimated_probs}')
print(f'Total reward: {total_reward}')
print(f'Optimal bandit: {np.argmax(true_probs)}')
print(f'Times each bandit was pulled: {num_times_pulled}')
```

### Output

We can see that this algorithm is able to find the optimal bandit as long as we have enough pulls.

```bash
Estimated probs: [0.1875     0.29166667 0.         0.26923077 0.72727273]
Total reward: 663
Optimal bandit: 4
Times each bandit was pulled: [ 48.  24.  22.  26. 880.]
```

## More Details

Epsilon-Greedy is just the beginning and can be extended in several ways:

1. Decay Epsilon: Gradually decrease the probability of exploration, allowing the algorithm to converge to the optimal solution over time.

2. Epsilon-Greedy with Softmax: Unlike standard Epsilon-Greedy, which explores uniformly, Softmax explores actions that are more likely to yield higher rewards.

3. Adjust Epsilon: The probability of exploration can be adjusted based on contextual information or confidence levels.

## References

- [Part 2: In-depth Exploration on Epsilon-greedy algorithm](https://medium.com/@gridflowai/part-2-in-depth-exploration-on-epsilon-greedy-algorithm-2b19e59bbe22)
